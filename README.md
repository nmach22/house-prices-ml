# house-prices-ml

## [Kaggle-ის კონკურსის მოკლე მიმოხილვა](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)

`მოცემული გვაქვს ბაზა, რომელშიც აღწერილია 79 მახასიათებელი სახლის შესახებ და თითოეულისთვის ცნობილია ფასი. ჩვენი ამოცანაა, მოცემული სატრენინგო ბაზის მიხედვით, ახალი სახლისთვის, რომლისთვისაც მოცემული გვექნება ის მახასიათებლები, რომლებიც მოცემულია პროექტში, დავადგინოთ გასაყიდი ფასი.`

## რეპოზიტორიის სტრუქტურა
```
house-prices-ml/
│
├── house-prices.ipynb    # ძირითადი source code
├── README.md             # პროექტის აღწერა
├── data/
│   ├── train.csv
│   └── test.csv
└── model_inference.ipynb # გაწვრთნილი მოდელის ჩამოტვირთვა mlflow დან და prediction ის დაგენერირება
```

## Feature Engineering
ყველა feature -თვის ავაგე გრაფიკი eda.ipynb ფაილში, საიდანაც ვიზიალურად ვეცადე დამეთვალიერებინა როგორ გამოიყურებოდა მოცემული დატასეტი.
ასევე გამოვიკვლიე N/A მნიშვნელობების რაოდენობა მთელიან მონაცემებში.
- თავიდან ყველა feature ში nan მნიშვნელობები შევცალე 0 ით ან კატეგორიული მახასიათებლებისთვის უბრალოდ 'nan' -ით.
- შემდეგ, კატეგორიული მახასითებლებისთვის ჩავთვალე რომ უკეთესი იქნებოდა თუ მოდას გამოვიყენებდი.
- feature ებში რამდენიმე სვეტი გვხვდება ისეთი, სადაც ძალიან ბევრი nan მნიშვნელობა წერია, თავიდან ვიფიქრე ამ მახასიათებლების საერთოდ წაშლა, თუმცა შემდეგ აღმოვაჩინე რომ nan-ს თავისი მნიშვნელობა ჰქონდა აღწერაში და უბრალოდ წაშლა არ იქნებოდა სწორი.
- ძირითადად გამოვიყენე One Hot Encoding თუმცა, ამასთან ერთად ვცადე ასევე TargetEncoder.

## Feature Selection
- rfe feature selection გამოვიყენე, ჰიპერ პარამეტრი n_features_to_select გადავარჩიე grid_search ის საშუალებით.
  

## Training
- ტესტირებული მოდელები:
  - პირველი (საცდელი) მიდგომა არის, გამოვთვალოთ საშუალო ფასი მოცემული სატრენინგო ბაზის მიხედვით და ყველა ახალი მონაცემისთვის დავაბრუნოთ ეს ფასი.
  - LinearRegression - შემდეგი მოდელი საცდელი მოდელი, სადაც ყველაზე კარგი შედეგი ჩემს მიერ გამოყოფილ სატესტო დატასეტზე მქონდა 0.12
    - ამ მოდელზე მუშაობისას, აღმოვაჩინე რომ მონაცემებში საკმაოდ ბევრი outlaier წერტილი გვყავდა. ამის დადგენაში დამეხმარა dataset ის დაყოფისას random_state ის ცვლილება. ამ ჰიპერ პარამეტრის ცვლას შედეგის საგრძნობი ცვლილება მოყვებოდა, არადა თითქოს ამას გავლენა არ უნდა ექონია ჩემს პასუხზე. ამ ყველაფერს განაპირობებს ისიც რომ train დატასეტში ცოტა რაოდენობის ჩანაწერი გვხვდება, სულ 1400 რიგი.
  - RansacRegression ბევრი outlaier წერტილის შემჩნევის შემდეგ მოვძებნე RansacRegression მოდელი, რომლის აღწერაშიც ეწერა, რომ იგი უფრო მეტად გამძლე იყო outlaier ების მიმართ.
    - ამ მოდელთან ერთად Recursive Feature Elimination არ მუშაობს, და ტრეინის დროსაც warnings ვარდება.
  - Lasso (L1 regularization), ამ მოდელით წინა წრფივი რეგრესიის მოდელი ვერ გავაუმჯობესე.
  - Ridge (L2 regularization), ამ მოდელით მივუახლოვდი წრფივს, მაგრამ წრფივის შედეგი ტესტსეტზე მაინც უკეთესი იყო, ვიდრე L2 regularization ით
  - Random Forest, ამ მოდელმა საუკეთესო შედეგი მომცა საბოლოო, კეგლის შეფასებაზე, თუმცა ბევრად დიდი დრო სჭირდება ამის დატრენინგებას, ვიდრე წრფივი მოდელების დატრენინგებას.
- Hyperparameter ოპტიმიზაციის მიდგომა:
  - grid search ის გამოყენებით გადავარჩიე ჰიპერპარამეტრები.
 
საბოლოოდ, ყველაზე კარგი მოდელი გამომივიდა Random Forest რომელმაც კეგლზე ყველაზე კარგი შედეგი აჩვენა.


## MLflow Tracking
- [model always return mean](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/0/runs/bba1e96e0c5849acbf494056965b50ad)
- [Linear Regression](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/2/runs/8b949a1397384ff3aa328e46f62516fc)
- [Lasso -L1 regularization](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/2/runs/846d73f52be045dcb6235bb116f8a876)
- [Ridge -L2 regularization](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/2/runs/d84b751807ac4a09b223ebd06433824c)
- [Random Forest](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/1/runs/ef457fdf335a4acba89583b60a9b8be6)
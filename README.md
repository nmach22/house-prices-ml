# house-prices-ml

## [Kaggle-ის კონკურსის მოკლე მიმოხილვა](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview)

- მოცემული გვაქვს ბაზა, რომელშიც აღწერილია 79 მახასიათებელი სახლის შესახებ და თითოეულისთვის ცნობილია ფასი. ჩვენი ამოცანაა, მოცემული სატრენინგო ბაზის მიხედვით, ახალი სახლისთვის, რომლისთვისაც მოცემული გვექნება ის მახასიათებლები, რომლებიც მოცემულია პროექტში, დავადგინოთ გასაყიდი ფასი.

- პირველი (საცდელი) მიდგომა არის, გამოვთვალოთ საშუალო ფასი მოცემული სატრენინგო ბაზის მიხედვით და ყველა ახალი მონაცემისთვის დავაბრუნოთ ეს ფასი.
- შემდეგი მოდელი ავიღე Linear Regression, სადაც ყველაზე კარგი შედეგი ჩემს მიერ გამოყოფილ სატესტო დატასეტზე მქონდა 0.12
  - ამ მოდელზე მუშაობისას, აღმოვაჩინე რომ მონაცემებში საკმაოდ ბევრი outlaier წერტილი გვყავდა. ამის დადგენაში დამეხმარა dataset ის დაყოფისას random_state ის ცვლილება. ამ ჰიპერ პარამეტრის ცვლას შედეგის საგრძნობი ცვლილება მოყვებოდა, არადა თითქოს ამას გავლენა არ უნდა ექონია ჩემს პასუხზე. ამ ყველაფერს განაპირობებს ისიც რომ train დატასეტში ცოტა რაოდენობის ჩანაწერი გვხვდება, სულ 1400 რიგი.
- ბევრი outlaier წერტილის შემჩნევის შემდეგ მოვძებნე RansacRegression მოდელი, რომლის აღწერაშიც ეწერა, რომ იგი უფრო მეტად გამძლე იყო outlaier ების მიმართ.
  - ამ მოდელზე რა გავაკეთე 
- შემდეგი მოდელი ვცადე Lasso 

## რეპოზიტორიის სტრუქტურა
```
house-prices-ml/
│
├── house-prices.ipynb    # ძირითადი source code
├── README.md             # პროექტის აღწერა
├── data/
│   ├── train.csv
│   └── test.csv
└── model_inference.ipynb # გაწვრთნილი მოდელის ჩამოტვირთვა mlflow დან და prediction ის დაგენერირება
```

## Feature Engineering
ყველა feature -თვის ავაგე გრაფიკი eda.ipynb ფაილში, საიდანაც ვიზიალურად ვეცადე დამეთვალიერებინა როგორ გამოიყურებოდა მოცემული დატასეტი.
ასევე გამოვიკვლიე N/A მნიშვნელობების რაოდენობა მთელიან მონაცემებში.
- თავიდან ყველა feature ში nan მნიშვნელობები შევცალე 0 ით ან კატეგორიული მახასიათებლებისთვის უბრალოდ 'nan' -ით.
- შემდეგ, კატეგორიული მახასითებლებისთვის ჩავთვალე რომ უკეთესი იქნებოდა თუ მოდას გამოვიყენებდი.
- feature ებში რამდენიმე სვეტი გვხვდება ისეთი, სადაც ძალიან ბევრი nan მნიშვნელობა წერია, თავიდან ვიფიქრე ამ მახასიათებლების საერთოდ წაშლა, თუმცა შემდეგ აღმოვაჩინე რომ nan-ს თავისი მნიშვნელობა ჰქონდა აღწერაში და უბრალოდ წაშლა არ იქნებოდა სწორი.
- 

## Feature Selection
- rfe feature selection გამოვიყენე, ჰიპერ პარამეტრი n_features_to_select გადავარჩიე grid_search ის საშუალებით.
  - Linear Regression params: 
    - rfe__n_features_to_select = 30
    - rfe__step = 1

## Training

## MLflow Tracking
- [model always return mean](https://dagshub.com/nmach22/house-prices-ml.mlflow/#/experiments/0/runs/bba1e96e0c5849acbf494056965b50ad)
  - rmse: 87619.04810843406
  - mae: 62576.058219178085
  - r2: -0.0008828026420326651
